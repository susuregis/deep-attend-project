# -*- coding: utf-8 -*-
"""Untitled11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19GjPwnpBUWd4pjsWyHSFUKVdK1R842ac
"""

#!pip install roboflow torch torchvision timm pyyaml scikit-learn matplotlib seaborn -q

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import torch
import torch.nn as nn
import timm
from torch.utils.data import Dataset
from pathlib import Path
import yaml
from torch.utils.data import DataLoader
from torchvision import transforms
from PIL import Image
import io
import uvicorn
import nest_asyncio
from datetime import datetime
import base64

from roboflow import Roboflow

rf = Roboflow(api_key="bJOCz45QyqwU2ubTQNxo")
project = rf.workspace("suelen").project("focus-of-attention-aujxc")
version = project.version(1)
dataset = version.download("yolov8")

class Config:
    # Dados
    img_size = 112

    # Treinamento
    batch_size = 64
    epochs = 10
    lr = 0.001

    # Hardware
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    num_workers = 2

    # Modelo
    backbone = 'mobilenetv3_small_100'
    hidden_lstm = 64

    #  'Focus-of-Attention-1' ou './Focus-of-Attention-1'
    dataset_path = 'Focus-of-Attention-1'

    classes = []
    num_classes = 0

cfg = Config()

print(f"  - Dataset: {cfg.dataset_path}")
print(f"  - Device: {cfg.device}")
print(f"  - Batch size: {cfg.batch_size}")
print(f"  - Épocas: {cfg.epochs}")

class RoboflowAttentionDataset(Dataset):
    """Dataset agrupado: Atento (Frontal) vs Desatento (resto)"""

    def __init__(self, dataset_path, split='train', transform=None):
        self.dataset_path = Path(dataset_path)
        self.split = split
        self.transform = transform

        if split == 'val':
            split = 'valid'

        self.img_dir = self.dataset_path / split / 'images'
        self.label_dir = self.dataset_path / split / 'labels'

        # Lê o data.yaml
        yaml_path = self.dataset_path / 'data.yaml'
        with open(yaml_path, 'r') as f:
            data_config = yaml.safe_load(f)

        self.original_classes = data_config['names']

        # MAPEIA PARA 2 CLASSES
        # 0 = Atento (Front Frontal)
        # 1 = Desatento (todas as outras)
        self.class_mapping = {}
        for idx, class_name in enumerate(self.original_classes):
            if 'frontal' in class_name.lower():
                self.class_mapping[idx] = 0  # ATENTO
            else:
                self.class_mapping[idx] = 1  # DESATENTO

        print(f"Mapeamento de classes:")
        for orig_idx, orig_name in enumerate(self.original_classes):
            new_class = "ATENTO" if self.class_mapping[orig_idx] == 0 else "DESATENTO"
            print(f"    {orig_name} → {new_class}")

        # Coleta amostras
        self.samples = []
        if self.img_dir.exists():
            for img_path in self.img_dir.glob('*.*'):
                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                    label_path = self.label_dir / f"{img_path.stem}.txt"
                    if label_path.exists():
                        with open(label_path, 'r') as f:
                            line = f.readline().strip()
                            if line:
                                original_class_id = int(line.split()[0])
                                # MAPEIA PARA NOVA CLASSE (0=Atento, 1=Desatento)
                                new_class_id = self.class_mapping[original_class_id]
                                self.samples.append((str(img_path), new_class_id))

        print(f"{split.upper()}: {len(self.samples)} imagens")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]

        img = cv2.imread(img_path)
        if img is None:
            img = np.zeros((224, 224, 3), dtype=np.uint8)
        else:
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        if self.transform:
            img = self.transform(img)

        return img, label

    def get_class_names(self):
        return ['Atento', 'Desatento']

def get_transforms(train=True):
    if train:
        return transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((cfg.img_size, cfg.img_size)),
            transforms.RandomHorizontalFlip(0.5),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
    else:
        return transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((cfg.img_size, cfg.img_size)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])

train_dataset = RoboflowAttentionDataset(
    cfg.dataset_path,
    split='train',
    transform=get_transforms(train=True)
)

# Validação
val_dataset = RoboflowAttentionDataset(
    cfg.dataset_path,
    split='valid',
    transform=get_transforms(train=False)
)

# Teste
test_dataset = RoboflowAttentionDataset(
    cfg.dataset_path,
    split='test',
    transform=get_transforms(train=False)
)

# Atualiza configurações
cfg.classes = train_dataset.get_class_names()
cfg.num_classes = len(cfg.classes)

print(f"Classes: {cfg.classes}")
print(f"Total de classes: {cfg.num_classes}")

# DataLoaders
train_loader = DataLoader(
    train_dataset,
    batch_size=cfg.batch_size,
    shuffle=True,
    num_workers=cfg.num_workers,
    pin_memory=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=cfg.batch_size,
    shuffle=False,
    num_workers=cfg.num_workers,
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=cfg.batch_size,
    shuffle=False,
    num_workers=cfg.num_workers,
    pin_memory=True
)


print(f"Train batches: {len(train_loader)}")
print(f"Val batches: {len(val_loader)}")
print(f"Test batches: {len(test_loader)}")

from collections import Counter

# Conta classes
train_labels = [label for _, label in train_dataset.samples]
class_counts = Counter(train_labels)

print(f"Distribuição (Atento/Desatento):")
for class_id in sorted(class_counts.keys()):
    count = class_counts[class_id]
    pct = (count / len(train_labels)) * 100
    print(f"  {cfg.classes[class_id]}: {count} ({pct:.1f}%)")

# Calcula pesos
total = len(train_labels)
weights = [total / (len(class_counts) * class_counts[i]) for i in sorted(class_counts.keys())]
class_weights_tensor = torch.FloatTensor(weights).to(cfg.device)

print(f"Pesos: {class_weights_tensor}")

class LightAttentionModel(nn.Module):
    """Modelo híbrido CNN + LSTM"""

    def __init__(self, num_classes=2, hidden_size=64):
        super().__init__()

        self.cnn = timm.create_model(
            cfg.backbone,
            pretrained=True,
            num_classes=0,
            global_pool=''
        )

        with torch.no_grad():
            dummy = torch.randn(1, 3, cfg.img_size, cfg.img_size)
            features = self.cnn(dummy)
            self.feature_size = features.shape[1]

        self.pool = nn.AdaptiveAvgPool2d(1)

        self.lstm = nn.LSTM(
            input_size=self.feature_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True
        )

        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, num_classes)
        )

    def forward(self, x):
        if len(x.shape) == 4:
            x = x.unsqueeze(1)

        batch_size, seq_len = x.shape[0], x.shape[1]
        x = x.view(batch_size * seq_len, *x.shape[2:])
        features = self.cnn(x)
        features = self.pool(features).squeeze(-1).squeeze(-1)
        features = features.view(batch_size, seq_len, -1)

        lstm_out, _ = self.lstm(features)
        lstm_out = lstm_out[:, -1, :]

        out = self.classifier(lstm_out)
        return out

model = LightAttentionModel(num_classes=cfg.num_classes, hidden_size=cfg.hidden_lstm)
model = model.to(cfg.device)

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"  - Total parâmetros: {total_params:,}")
print(f"  - Treináveis: {trainable_params:,}")

def evaluate_model(model, dataloader, split_name='Test'):
    """Avalia o modelo e retorna métricas detalhadas"""
    model.eval()

    all_preds = []
    all_labels = []
    all_probs = []



    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc=f'Eval {split_name}'):
            images = images.to(cfg.device)
            labels = labels.to(cfg.device)

            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    # Métricas
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='weighted'
    )


    print(f"Resultados {split_name.upper()}")
    print(f"  Acurácia:  {accuracy*100:.2f}%")
    print(f"  Precisão:  {precision*100:.2f}%")
    print(f"  Recall:    {recall*100:.2f}%")
    print(f"  F1-Score:  {f1*100:.2f}%")

    # Relatório por classe
    print(f"Relatório por Classe:")
    print(classification_report(
        all_labels, all_preds,
        target_names=cfg.classes,
        digits=4
    ))

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'predictions': all_preds,
        'labels': all_labels,
        'probabilities': all_probs
    }


def plot_confusion_matrix(results, split_name='Test'):
    """Plota matriz de confusão"""
    cm = confusion_matrix(results['labels'], results['predictions'])

    plt.figure(figsize=(8, 6))
    sns.heatmap(
        cm, annot=True, fmt='d', cmap='Blues',
        xticklabels=cfg.classes,
        yticklabels=cfg.classes
    )
    plt.title(f'Matriz de Confusão - {split_name}')
    plt.ylabel('Verdadeiro')
    plt.xlabel('Predito')
    plt.tight_layout()
    plt.show()

    # Calcula métricas por classe
    print(f"Métricas da Matriz de Confusão:")
    for i, class_name in enumerate(cfg.classes):
        tp = cm[i, i]
        fp = cm[:, i].sum() - tp
        fn = cm[i, :].sum() - tp
        tn = cm.sum() - tp - fp - fn

        class_acc = (tp + tn) / cm.sum() if cm.sum() > 0 else 0
        class_prec = tp / (tp + fp) if (tp + fp) > 0 else 0
        class_rec = tp / (tp + fn) if (tp + fn) > 0 else 0

        print(f"Classe '{class_name}':")
        print(f"    TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}")
        print(f"    Acurácia: {class_acc*100:.2f}%")
        print(f"    Precisão: {class_prec*100:.2f}%")
        print(f"    Recall: {class_rec*100:.2f}%")

from torch import optim
import time
from tqdm import tqdm
import cv2

def train_model():

    # Setup
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)
    optimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)
    scaler = torch.cuda.amp.GradScaler() if cfg.device == 'cuda' else None

    # Histórico
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }

    best_val_acc = 0
    start_time = time.time()

    # Loop de épocas
    for epoch in range(cfg.epochs):
        print(f"Época {epoch+1}/{cfg.epochs}")


        # treino
        model.train()
        train_loss = 0
        correct = 0
        total = 0

        pbar = tqdm(train_loader, desc='Treinando')
        for images, labels in pbar:
            images = images.to(cfg.device)
            labels = labels.to(cfg.device)

            optimizer.zero_grad()

            if scaler:
                with torch.cuda.amp.autocast():
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

            acc = 100. * correct / total
            pbar.set_postfix({'loss': f'{loss.item():.3f}', 'acc': f'{acc:.1f}%'})

        train_acc = 100. * correct / total
        train_loss = train_loss / len(train_loader)

        # validação
        model.eval()
        val_loss = 0
        correct = 0
        total = 0

        with torch.no_grad():
            for images, labels in tqdm(val_loader, desc='Validando'):
                images = images.to(cfg.device)
                labels = labels.to(cfg.device)

                outputs = model(images)
                loss = criterion(outputs, labels)

                val_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()

        val_acc = 100. * correct / total
        val_loss = val_loss / len(val_loader)

        # Atualiza histórico
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        # Print resultados
        print(f"Treino    - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%")
        print(f"Validação - Loss: {val_loss:.4f} | Acc: {val_acc:.2f}%")

        # Salva melhor modelo
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_attention_model.pth')
            print(f"Melhor modelo salvo (Acc: {val_acc:.2f}%)")

        scheduler.step(val_acc)

    elapsed = time.time() - start_time


    print(f" Melhor Val Acc: {best_val_acc:.2f}%")
    print(f"Modelo salvo: best_attention_model.pth")

    return history

history = train_model()

from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns




model.load_state_dict(torch.load('best_attention_model.pth'))
test_results = evaluate_model(model, test_loader, 'Teste')
plot_confusion_matrix(test_results, 'Teste')

def plot_training_history(history):
    """Plota gráficos de loss e acurácia"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

    epochs = range(1, len(history['train_loss']) + 1)

    # Loss
    ax1.plot(epochs, history['train_loss'], 'b-', label='Treino', linewidth=2)
    ax1.plot(epochs, history['val_loss'], 'r-', label='Validação', linewidth=2)
    ax1.set_title('Loss por Época', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Época')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Acurácia
    ax2.plot(epochs, history['train_acc'], 'b-', label='Treino', linewidth=2)
    ax2.plot(epochs, history['val_acc'], 'r-', label='Validação', linewidth=2)
    ax2.set_title('Acurácia por Época', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Época')
    ax2.set_ylabel('Acurácia (%)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# Plota histórico
plot_training_history(history)

model.load_state_dict(torch.load('best_attention_model.pth'))

# Avalia no treino
train_results = evaluate_model(model, train_loader, 'Treino')

# Matriz de confusão
plot_confusion_matrix(train_results, 'Treino')

# Avalia na validação
val_results = evaluate_model(model, val_loader, 'Validação')

# Matriz de confusão
plot_confusion_matrix(val_results, 'Validação')

def evaluate_with_tta(model, dataloader, split_name='Test', tta_transforms=5):
    """Avalia com Test-Time Augmentation para maior acurácia"""
    model.eval()

    all_preds = []
    all_labels = []

    print(f"Avaliando {split_name} com TTA ({tta_transforms} versões)...")

    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc=f'TTA {split_name}'):
            batch_probs = []

            # Original
            imgs = images.to(cfg.device)
            outputs = model(imgs)
            probs = torch.softmax(outputs, dim=1)
            batch_probs.append(probs)

            # Flip horizontal
            imgs_flip = torch.flip(images, dims=[3]).to(cfg.device)
            outputs_flip = model(imgs_flip)
            probs_flip = torch.softmax(outputs_flip, dim=1)
            batch_probs.append(probs_flip)

            # Pequenas variações (se tta_transforms > 2)
            if tta_transforms > 2:
                # Brilho +10%
                imgs_bright = (images * 1.1).clamp(0, 1).to(cfg.device)
                outputs_bright = model(imgs_bright)
                batch_probs.append(torch.softmax(outputs_bright, dim=1))

            if tta_transforms > 3:
                # Brilho -10%
                imgs_dark = (images * 0.9).to(cfg.device)
                outputs_dark = model(imgs_dark)
                batch_probs.append(torch.softmax(outputs_dark, dim=1))

            if tta_transforms > 4:
                # Contraste
                imgs_contrast = ((images - 0.5) * 1.2 + 0.5).clamp(0, 1).to(cfg.device)
                outputs_contrast = model(imgs_contrast)
                batch_probs.append(torch.softmax(outputs_contrast, dim=1))

            # Média das probabilidades
            avg_probs = torch.stack(batch_probs).mean(dim=0)
            _, predicted = avg_probs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.numpy())

    # Métricas
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(
        all_labels, all_preds, average='weighted'
    )

    print(f"\n{'='*50}")
    print(f"Resultados {split_name.upper()} com TTA")
    print(f"{'='*50}")
    print(f"  Acurácia:  {accuracy*100:.2f}%")
    print(f"  Precisão:  {precision*100:.2f}%")
    print(f"  Recall:    {recall*100:.2f}%")
    print(f"  F1-Score:  {f1*100:.2f}%")

    # Relatório por classe
    print(f"\nRelatório por Classe:")
    print(classification_report(
        all_labels, all_preds,
        target_names=cfg.classes,
        digits=4
    ))

    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'predictions': all_preds,
        'labels': all_labels,
        'probabilities': None
    }

test_results = evaluate_with_tta(model, test_loader, 'Teste', tta_transforms=5)

# Plota matriz
plot_confusion_matrix(test_results, 'Teste')


# Sem TTA
test_results_normal = evaluate_model(model, test_loader, 'Teste (Sem TTA)')

print(f"  Sem TTA: {test_results_normal['accuracy']*100:.2f}%")
print(f"  Com TTA: {test_results['accuracy']*100:.2f}%")
print(f"  Ganho:   {(test_results['accuracy'] - test_results_normal['accuracy'])*100:+.2f}%")

import numpy as np


print(f"Comparação - Treino vs teste ")


comparison_data = {
    'Conjunto': ['Treino', 'Validação', 'Teste'],
    'Acurácia (%)': [
        train_results['accuracy']*100,
        val_results['accuracy']*100,
        test_results['accuracy']*100
    ],
    'Precisão (%)': [
        train_results['precision']*100,
        val_results['precision']*100,
        test_results['precision']*100
    ],
    'Recall (%)': [
        train_results['recall']*100,
        val_results['recall']*100,
        test_results['recall']*100
    ],
    'F1-Score (%)': [
        train_results['f1']*100,
        val_results['f1']*100,
        test_results['f1']*100
    ]
}

# Tabela formatada
print(f"\n{'Conjunto':<15} {'Acurácia':<12} {'Precisão':<12} {'Recall':<12} {'F1-Score':<12}")
print(f"{'-'*63}")
for i in range(3):
    print(f"{comparison_data['Conjunto'][i]:<15} "
          f"{comparison_data['Acurácia (%)'][i]:>10.2f}% "
          f"{comparison_data['Precisão (%)'][i]:>10.2f}% "
          f"{comparison_data['Recall (%)'][i]:>10.2f}% "
          f"{comparison_data['F1-Score (%)'][i]:>10.2f}%")

# Gráfico comparativo
fig, ax = plt.subplots(figsize=(12, 6))
x = np.arange(len(comparison_data['Conjunto']))
width = 0.2

ax.bar(x - 1.5*width, comparison_data['Acurácia (%)'], width, label='Acurácia', color='#3498db')
ax.bar(x - 0.5*width, comparison_data['Precisão (%)'], width, label='Precisão', color='#2ecc71')
ax.bar(x + 0.5*width, comparison_data['Recall (%)'], width, label='Recall', color='#e74c3c')
ax.bar(x + 1.5*width, comparison_data['F1-Score (%)'], width, label='F1-Score', color='#f39c12')

ax.set_xlabel('Conjunto de Dados', fontweight='bold', fontsize=12)
ax.set_ylabel('Porcentagem (%)', fontweight='bold', fontsize=12)
ax.set_title('Comparação de Métricas - Treino vs Validação vs Teste', fontweight='bold', fontsize=14)
ax.set_xticks(x)
ax.set_xticklabels(comparison_data['Conjunto'])
ax.legend()
ax.grid(True, alpha=0.3, axis='y')
ax.set_ylim([0, 105])

plt.tight_layout()
plt.show()


train_acc = train_results['accuracy']*100
val_acc = val_results['accuracy']*100
test_acc = test_results['accuracy']*100

diff_train_val = train_acc - val_acc
diff_val_test = val_acc - test_acc

print(f"Treino vs Validação: {diff_train_val:+.2f}%")
if diff_train_val > 10:
    print(" possível overfitting forte.")
elif diff_train_val > 5:
    print(" possível início de overfitting.")
else:
    print(" modelo generalizando bem.")

print(f"\n  Validação vs Teste: {diff_val_test:+.2f}%")
if abs(diff_val_test) > 5:
    print(" distribuição dos dados pode estar diferente.")
else:
    print("desempenho consistente.")

print(f" Acurácia Final no Teste: {test_acc:.2f}%")
if test_acc >= 90:
    print("  Excelente")
elif test_acc >= 80:
    print("  Bom desempenho")
elif test_acc >= 70:
    print("  Razoável")
else:
    print("  Desempenho baixo")

# Contagem por conjunto
print(f"Tamanho dos conjuntos:")
print(f"  Treino: {len(train_dataset)} imagens")
print(f"  Validação: {len(val_dataset)} imagens")
print(f"  Teste: {len(test_dataset)} imagens")

# Balanceamento
from collections import Counter

def check_balance(dataset, name):
    labels = [label for _, label in dataset.samples]
    counter = Counter(labels)
    print(f"Distribuição {name}:")
    for class_id, count in counter.items():
        class_name = cfg.classes[class_id]
        percentage = (count / len(labels)) * 100
        print(f"  {class_name}: {count} ({percentage:.1f}%)")

    # Verifica desbalanceamento
    values = list(counter.values())
    ratio = max(values) / min(values) if min(values) > 0 else float('inf')
    if ratio > 1.5:
        print(f" desbalanceado: {ratio:.2f}:1")
    else:
        print(f"Bem balanceado")

check_balance(train_dataset, "Treino")
check_balance(val_dataset, "Validação")
check_balance(test_dataset, "Teste")

# Visualiza amostras
import random

fig, axes = plt.subplots(2, 4, figsize=(16, 8))
for i, ax in enumerate(axes.flat):
    idx = random.randint(0, len(train_dataset)-1)
    img, label = train_dataset[idx]

    # Desnormaliza para visualizar
    img = img.numpy().transpose(1, 2, 0)
    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
    img = np.clip(img, 0, 1)

    ax.imshow(img)
    ax.set_title(f"Classe: {cfg.classes[label]}")
    ax.axis('off')
plt.tight_layout()
plt.show()

import json
from datetime import datetime

def save_model_info():
    """Salva todas as informações do modelo"""

    info = {
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'model': {
            'backbone': cfg.backbone,
            'hidden_lstm': cfg.hidden_lstm,
            'num_classes': cfg.num_classes,
            'classes': cfg.classes,
            'img_size': cfg.img_size,
            'total_params': sum(p.numel() for p in model.parameters()),
        },
        'training': {
            'epochs': cfg.epochs,
            'batch_size': cfg.batch_size,
            'learning_rate': cfg.lr,
            'optimizer': 'AdamW',
            'scheduler': 'CosineAnnealingLR',
            'loss': 'CrossEntropyLoss (weighted)',
            'class_weights': class_weights_tensor.tolist(),
        },
        'results': {
            'train': {
                'accuracy': float(train_results['accuracy'] * 100),
                'precision': float(train_results['precision'] * 100),
                'recall': float(train_results['recall'] * 100),
                'f1': float(train_results['f1'] * 100),
            },
            'validation': {
                'accuracy': float(val_results['accuracy'] * 100),
                'precision': float(val_results['precision'] * 100),
                'recall': float(val_results['recall'] * 100),
                'f1': float(val_results['f1'] * 100),
            },
            'test': {
                'accuracy': float(test_results['accuracy'] * 100),
                'precision': float(test_results['precision'] * 100),
                'recall': float(test_results['recall'] * 100),
                'f1': float(test_results['f1'] * 100),
            },
        },
        'files': {
            'pytorch_model': 'best_attention_model.pth',
            'onnx_model': 'attention_model.onnx',
            'config': 'model_config.json',
        }
    }

    # Salva JSON
    with open('model_config.json', 'w') as f:
        json.dump(info, f, indent=2)

    print("Configurações salvas em: model_config.json")
    print(json.dumps(info, indent=2))

# Antes de salvar, precisamos dos resultados de treino e validação
print("Avaliando treino e validação...")
train_results = evaluate_model(model, train_loader, 'Treino')
val_results = evaluate_model(model, val_loader, 'Validação')


save_model_info()

import json
import os
import torch


#  Criar pasta para os arquivos
#!mkdir -p deploy_files
os.chdir('deploy_files')

# Salvar modelo treinado

torch.save(model.state_dict(), 'model.pth')
print(f"model.pth salvo ({os.path.getsize('model.pth')/1024/1024:.1f} MB)\n")

# 3. Salvar configurações


config_data = {
    'img_size': cfg.img_size,
    'classes': cfg.classes,
    'num_classes': cfg.num_classes,
    'backbone': cfg.backbone,
    'hidden_lstm': cfg.hidden_lstm,
    'accuracy': '72.30%',
    'model_type': 'CNN + LSTM'
}

with open('config.json', 'w') as f:
    json.dump(config_data, f, indent=2)

print("config.json salvo\n")

# Criar app.py


app_code = '''
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import torch
import torch.nn as nn
import timm
from torchvision import transforms
from PIL import Image
import io
import json
from datetime import datetime

# Carregar configuracoes
with open('config.json', 'r') as f:
    CONFIG = json.load(f)

# Definicao do modelo
class LightAttentionModel(nn.Module):
    def __init__(self, num_classes=2, hidden_size=64):
        super().__init__()

        self.cnn = timm.create_model(
            CONFIG['backbone'],
            pretrained=False,
            num_classes=0,
            global_pool=''
        )

        with torch.no_grad():
            dummy = torch.randn(1, 3, CONFIG['img_size'], CONFIG['img_size'])
            features = self.cnn(dummy)
            self.feature_size = features.shape[1]

        self.pool = nn.AdaptiveAvgPool2d(1)

        self.lstm = nn.LSTM(
            input_size=self.feature_size,
            hidden_size=hidden_size,
            num_layers=1,
            batch_first=True
        )

        self.classifier = nn.Sequential(
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(32, num_classes)
        )

    def forward(self, x):
        if len(x.shape) == 4:
            x = x.unsqueeze(1)

        batch_size, seq_len = x.shape[0], x.shape[1]
        x = x.view(batch_size * seq_len, *x.shape[2:])
        features = self.cnn(x)
        features = self.pool(features).squeeze(-1).squeeze(-1)
        features = features.view(batch_size, seq_len, -1)

        lstm_out, _ = self.lstm(features)
        lstm_out = lstm_out[:, -1, :]

        return self.classifier(lstm_out)

# Carregar modelo treinado
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
model = LightAttentionModel(
    num_classes=CONFIG['num_classes'],
    hidden_size=CONFIG['hidden_lstm']
)
model.load_state_dict(torch.load('model.pth', map_location=DEVICE))
model.to(DEVICE)
model.eval()

# FastAPI
app = FastAPI(
    title="Detector de Atencao",
    description="API para detecao de atencao de alunos",
    version="1.0.0"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Transform
transform = transforms.Compose([
    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

request_count = 0

@app.get("/")
def root():
    return {
        "message": "API de Deteccao de Atencao",
        "status": "online",
        "model": CONFIG['model_type'],
        "accuracy": CONFIG['accuracy'],
        "classes": CONFIG['classes'],
        "device": DEVICE,
        "requests_processed": request_count
    }

@app.get("/health")
def health():
    return {
        "status": "healthy",
        "device": DEVICE,
        "model_loaded": True,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    global request_count
    request_count += 1

    try:
        if not file.content_type.startswith('image/'):
            raise HTTPException(400, "Arquivo deve ser uma imagem")

        contents = await file.read()
        image = Image.open(io.BytesIO(contents)).convert('RGB')

        img_tensor = transform(image).unsqueeze(0).to(DEVICE)

        with torch.no_grad():
            output = model(img_tensor)
            prob = torch.softmax(output, dim=1)
            pred_class = prob.argmax(1).item()
            confidence = prob[0][pred_class].item()

        return JSONResponse(content={
            "success": True,
            "atento": pred_class == 0,
            "classe": CONFIG['classes'][pred_class],
            "confianca": round(float(confidence), 4),
            "probabilidades": {
                CONFIG['classes'][0]: round(float(prob[0][0]), 4),
                CONFIG['classes'][1]: round(float(prob[0][1]), 4)
            },
            "timestamp": datetime.now().isoformat()
        })

    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7860)
'''

with open('app.py', 'w') as f:
    f.write(app_code)

print("app.py criado\n")

# Criar requirements.txt


requirements = """
fastapi
uvicorn
torch
torchvision
pillow
timm
numpy
"""

with open('requirements.txt', 'w') as f:
    f.write(requirements)

print("requirements.txt criado\n")